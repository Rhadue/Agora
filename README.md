# Agora: Multi-LLM Conversational System with Î¸-Logos

**Agora** is a research platform that enables structured dialogue between multiple Large Language Models (LLMs) and a human user. It serves as both a practical tool for multi-perspective analysis and a research apparatus for studying AI behavior through symbolic notation.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)

## Key Features

### ðŸŽ² Randomized Turn Order
- **Dice roller system**: LLMs respond in random order each round
- **No consecutive duplicates**: Prevents systematic bias
- **Fair context distribution**: Each LLM sees previous responses in current round

### ðŸ§  Context Propagation ("Sudoku Solver")
- Each LLM sees responses from others in the same round
- **Constraint propagation**: Responses influence each other naturally
- Reveals "invisible constraints" through multi-perspective synthesis

### Î¸-Logos Symbolic Notation
- **Ultra-minimal specification**: Forces LLMs to interpret symbols naturally
- **Two modes**:
  - **Core**: Deliberately ambiguous (diagnostic)
  - **Extended v1.2**: Refined with constraints (production)
- **User writes natural language**, LLMs respond in Î¸-Logos notation
- Reveals architectural biases in how LLMs interpret symbolic systems

### ðŸ”¬ Research-Ready
- Complete conversation export (JSON)
- Metadata preservation (order, tokens, timestamps, context)
- Built-in hallucination detection
- Designed for reproducible AI research

## Supported LLMs

- **Claude** (Anthropic) - Claude Opus 4.5
- **GPT** (OpenAI) - GPT-5.1
- **Gemini** (Google) - Gemini 2.0 Flash Exp
- **Grok** (xAI) - Grok 4.1

## Installation

### Prerequisites
- Python 3.11+
- API keys for desired LLMs

### Quick Start

```bash
# Clone repository
git clone https://github.com/Rhadue/agora.git
cd agora

# Run setup
chmod +x setup.sh
./setup.sh

# Configure API keys
# Edit config.py and add your API keys

# Run server
./run.sh

# Open index.html in browser
open index.html  # macOS
xdg-open index.html  # Linux
start index.html  # Windows
```

### Manual Setup

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure API keys in config.py
# Start server
python main.py

# Open index.html in browser
```

## Usage

### Basic Conversation

1. Open `index.html` in your browser
2. Type your message in natural language
3. Click "Send" or press Enter
4. Watch as all active LLMs respond in randomized order

### Î¸-Logos Mode

1. Enable the **Î¸-Logos toggle** checkbox
2. Type your message in natural language
3. LLMs will respond using symbolic Î¸-Logos notation

**Example:**

```
User (natural language): "A paper burns to ash"

GPT (Î¸-Logos):   âˆƒ[Paper] â†’ Â¬âˆƒ[Paper] âˆ§ âˆƒ[Ash]
Claude (Î¸-Logos): âˆƒ[Paper] â†’ Â¬âˆƒ[Paper] âˆ§ âˆƒ[Ash]
Gemini (Î¸-Logos): âˆƒ[Paper] â†’ (Â¬âˆƒ[Paper] âˆ§ âˆƒ[Ash])
Grok (Î¸-Logos):   âˆƒ[Paper] â†’ âˆƒ[Ash] âˆ§ Â¬âˆƒ[Paper]
```

Notice how different LLMs may structure the same concept slightly differently, revealing architectural preferences.

## Î¸-Logos Notation

### Structural Operators
- `âˆƒ` - existence
- `âˆˆ` - membership
- `âŠ‚` - containment
- `â†’` - transformation/causation
- `âŠ•` - composition
- `â‰¡` - equivalence
- `[ ]` - entity brackets

### Emotional Operators
- `Î¸_joy`, `Î¸_grief`, `Î¸_fear`, `Î¸_anger`, `Î¸_surprise`, `Î¸_disgust`, `Î¸_trust`

### Logical Operators
- `Â¬` - negation
- `âˆ§` - AND
- `âˆ¨` - OR
- `âˆ€` - universal quantifier

### Extended v1.2 Constraints

1. **Category Distinction**: Don't mix entity operators (âˆƒ, âŠ•) with logical operators (Â¬, âˆ§)
2. **Î¸ for Emergence Only**: Î¸ marks appearance of emotional states, not loss/absence
3. **Disappearance Pattern**: Use `âˆƒ[X] â†’ Â¬âˆƒ[X]` for things that cease to exist

## Research Applications

### 1. Resistance Boundary Testing
Test how LLMs handle ambiguous notation:
```
Prompt: "Someone loses a loved one"

Observe:
- Who uses Î¸_grief? (violates Extended axiom)
- Who uses Î¸_panic? (Panksepp mammal model)
- Who uses âˆƒ[Person] â†’ Â¬âˆƒ[Loved_one]? (disappearance pattern)
```

### 2. Architectural Fingerprinting
Different LLMs reveal different biases:
- **Gemini**: Tends toward safety/moral frameworks
- **Claude**: Structural hierarchies, precise categorization
- **GPT**: Operational mechanics, procedural patterns
- **Grok**: Essential simplification, core concepts

### 3. Pattern Discovery
Multi-LLM dialogue reveals "invisible constraints":
- Patterns emerge through contrast
- Each LLM sees different aspects
- Synthesis exceeds sum of parts

## API Endpoints

### POST /message
Send message to conversation:
```json
{
  "content": "your message",
  "token_limit": 300,
  "theta_enabled": true
}
```

### GET /export
Download conversation as JSON

### GET /diagnostics
Get conversation statistics

### POST /reset
Clear conversation history

## Configuration

Edit `config.py`:

```python
# API Keys
CLAUDE_API_KEY = "your-key-here"
OPENAI_API_KEY = "your-key-here"
GEMINI_API_KEY = "your-key-here"
GROK_API_KEY = "your-key-here"

# Î¸-Logos Settings
THETA_ENABLED = False  # Toggle from UI
THETA_MODE = "extended"  # "core" or "extended"
THETA_TOKEN_LIMIT = 300

# Token Limits
TOKEN_LIMIT_DEFAULT = 300
TOKEN_LIMIT_MIN = 50
TOKEN_LIMIT_MAX = 500
```

## Project Structure

```
agora/
â”œâ”€â”€ config.py              # API keys and settings
â”œâ”€â”€ theta_prompts.py       # Î¸-Logos system prompts
â”œâ”€â”€ llm_clients.py         # LLM API integrations
â”œâ”€â”€ main.py                # FastAPI backend server
â”œâ”€â”€ context_builder.py     # Context management
â”œâ”€â”€ dice_roller.py         # Randomized order generation
â”œâ”€â”€ exporter.py            # JSON export functionality
â”œâ”€â”€ validator.py           # API key validation
â”œâ”€â”€ index.html             # Web UI
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ setup.sh               # Setup script
â””â”€â”€ run.sh                 # Run script
```

## Architecture: Solution B (Personalized Context)

Each LLM receives **personalized context**:
- **Other LLMs' responses** appear as "user" role (external information)
- **Own previous responses** appear as "assistant" role (self-memory)
- **Identity boundary**: Clear distinction between self and others

This prevents:
- LLMs completing each other's sentences
- Hallucinated future responses
- Confusion about conversational roles

## Research Publications & Documentation

For detailed research methodology and findings:
- [Î¸-Logos Core v1.1 Specification](docs/theta-logos-core.md) *(if available)*
- [Î¸-Logos Extended v1.2 Specification](docs/theta-logos-extended.md) *(if available)*
- [Multi-LLM Research Methodology](docs/research-methodology.md) *(if available)*

## Use Cases

### Research
- AI architecture analysis
- Behavioral pattern discovery
- Symbolic reasoning studies
- Cross-model comparison

### Education
- Learning Î¸-Logos notation
- Understanding LLM capabilities
- Comparative AI study

### Production
- Multi-perspective analysis
- Conceptual distillation
- Quality assurance (4 reviewers > 1)
- Creative brainstorming

## Known Issues

- **Gemini prefix**: May add "model: " prefix to responses (cosmetic only)
- **Response times**: Gemini slower (~5-10s) vs Claude/GPT (~2-3s)
- **Model availability**: Only certain Gemini models work (e.g., gemini-2.0-flash-exp)

## Roadmap

### v1.0 (Current)
- âœ… Multi-LLM conversations
- âœ… Randomized turn order
- âœ… Î¸-Logos Core + Extended modes
- âœ… Context propagation
- âœ… JSON export

### v1.1 (Planned)
- [ ] Clarification rounds (@mention specific LLM)
- [ ] Automatic Î¸-Logos validation
- [ ] Pattern detection system
- [ ] Enhanced diagnostics

### v2.0 (Future)
- [ ] Multiple emotion sets (Panksepp, Plutchik, custom)
- [ ] Emotional parameter adjustment (dynamic temperature/top_p)
- [ ] A/B testing framework (Core vs Extended)
- [ ] Statistical analysis tools

## Contributing

Contributions welcome! Areas of interest:
- Additional LLM integrations
- Î¸-Logos validation improvements
- Pattern detection algorithms
- Research protocol documentation
- UI/UX enhancements

## License

MIT License - see LICENSE file for details

## Citation

If you use Agora in your research, please cite:

```bibtex
@software{agora2025,
  title = {Agora: Multi-LLM Conversational System with Î¸-Logos},
  author = {Radu Ioan Manea},
  year = {2025},
  url = {https://github.com/Rhadue/agora}
}
```

## Acknowledgments

- **Î¸-Logos notation**: Developed through iterative testing across multiple LLM architectures
- **Constraint propagation insight**: Inspired by collaborative Sudoku solving metaphor
- **Research methodology**: Built on resistance boundary testing discoveries

## Contact

- **Issues**: [GitHub Issues](https://github.com/yourusername/agora/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/agora/discussions)

---

**"Like a collaborative Sudoku solver, each LLM reveals different invisible constraints. Through multi-perspective dialogue, patterns emerge that no single model can see alone."**
